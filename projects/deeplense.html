<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Specific Test VI - Foundation Model</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 20px auto;
      padding: 2rem;
      background-color: #f9f9f9;
      color: #333;
    }
    h2, h3, h4, h5 {
      color: #2c3e50;
    }
    code, pre {
      background: #eee;
      padding: 0.5em;
      display: block;
      white-space: pre-wrap;
      border-left: 4px solid #3498db;
    }
    img {
      max-width: 100%;
      height: auto;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1rem 0;
    }
    td, th {
      padding: 0.5rem;
      text-align: center;
      border: 1px solid #ccc;
    }
    .img-row {
      display: flex;
      gap: 1rem;
      flex-wrap: wrap;
      justify-content: center;
    }
    .img-row img {
      flex: 1 1 auto;
      max-width: 30%;
    }
  </style>
</head>
<body>

  <h2>Specific Test VI - Foundation Model</h2>
  <p>This folder contains my solution for <strong>Specific Test VI: Foundation Model</strong> of the DeepLense GSoC 2025 project. The task involves pretraining a <strong>Masked Autoencoder (MAE)</strong> on strong lensing images and fine-tuning it for <strong>multi-class classification</strong> and <strong>super-resolution</strong> using <strong>PyTorch</strong>.</p>

  <h3>üìå Task Overview</h3>
  <ol>
    <li><strong>Pretraining a Masked Autoencoder (MAE)</strong> on <strong>no_sub</strong> samples to learn meaningful feature representations.</li>
    <li>Fine-tuning the MAE:
      <ul>
        <li>For <strong>multi-class classification</strong> (distinguishing between no_sub, cdm, and axion).</li>
        <li>For <strong>super-resolution</strong> (upscaling low-resolution images using high-resolution ground truths).</li>
      </ul>
    </li>
  </ol>

  <h4>üì∑ Sample Images for Each Task</h4>
  <p><strong>Samples for multi-class classification</strong></p>
  <img src="../assets/deeplense/classification/classSample.png" alt="Classification Sample">

  <p><strong>Samples for super-resolution</strong></p>
  <img src="../assets/deeplense/superresolution/superRsample.png" alt="Super-resolution Sample">

  <h3>Prepare Data for Masked Autoencoder (MAE) Pretraining</h3>
  <h4>Input for Encoder</h4>
  <p><strong>Sample for splitted-image</strong></p>
  <img src="../assets/deeplense/mae/splitted_image.png" alt="Splitted Image">

  <p><strong>Sample for masked-image</strong></p>
  <img src="../assets/deeplense/mae/masked_image.png" alt="Masked Image">

  <h4>Masked patches and Visible patches</h4>
  <table>
    <tr>
      <td><img src="../assets/deeplense/mae/masked_patches.png" alt="Masked Patches"></td>
      <td><img src="../assets/deeplense/mae/visible_patches.png" alt="Visible Patches"></td>
    </tr>
  </table>

  <h3>üõ† Model and Approach</h3>
  <h4>1Ô∏è‚É£ Masked Autoencoder (MAE) Pretraining</h4>
  <ul>
    <li><strong>Goal:</strong> Learn a feature representation of strong lensing images.</li>
    <li><strong>Architecture:</strong> Vision Transformer (ViT) backbone with a reconstruction head.</li>
    <li><strong>Pretraining Loss:</strong> Mean Squared Error (MSE)</li>
    <li><strong>Optimizer:</strong> AdamW</li>
    <li><strong>Batch Size:</strong> 256</li>
    <li><strong>Epochs:</strong> 250</li>
  </ul>

  <h4>2Ô∏è‚É£ Fine-Tuning for Multi-Class Classification</h4>
  <ul>
    <li><strong>Loss Function:</strong> Cross-Entropy Loss</li>
    <li><strong>Optimizer:</strong> AdamW</li>
    <li><strong>Batch Size:</strong> 256</li>
    <li><strong>Evaluation Metrics:</strong> AUC Score, Accuracy</li>
    <li><strong>Epochs:</strong> 250</li>
  </ul>

  <h4>3Ô∏è‚É£ Fine-Tuning for Super-Resolution</h4>
  <ul>
    <li><strong>Loss Function:</strong> Mean Squared Error (MSE)</li>
    <li><strong>Batch Size:</strong> 256</li>
    <li><strong>Evaluation Metrics:</strong> MSE, SSIM, PSNR</li>
    <li><strong>Epochs:</strong> 200</li>
    <li><strong>NOTE:</strong> The decoder used here is not suitable for super-resolution tasks. Architectural improvements are needed.</li>
  </ul>

  <h3>üìä Results</h3>
  <h4>1Ô∏è‚É£ MAE Pretraining</h4>
  <img src="../assets/deeplense/mae/MAE_Losses.png" alt="MAE Loss">
  <table>
    <tr>
      <td><img src="../assets/deeplense/mae/pca_plot.png" alt="PCA Plot"></td>
      <td><img src="../assets/deeplense/mae/tsne_plot.png" alt="TSNE Plot"></td>
    </tr>
  </table>

  <h4>2Ô∏è‚É£ Multi-Class Classification</h4>
  <table>
    <tr>
      <td><img src="../assets/deeplense/classification/Accuracies.png" alt="Accuracy"></td>
      <td><img src="../assets/deeplense/classification/AUC.png" alt="AUC"></td>
    </tr>
  </table>

  <h5>Classification Report</h5>
  <pre>
              precision    recall  f1-score   support

      no_sub       0.97      0.99      0.98      2945
       axion       0.98      0.97      0.97      2990
         cdm       0.97      0.95      0.96      2976

    accuracy                           0.97      8911
   macro avg       0.97      0.97      0.97      8911
weighted avg       0.97      0.97      0.97      8911
  </pre>
  <table>
    <tr>
      <td><img src="../assets/deeplense/classification/ROC_curve.png" alt="ROC Curve"></td>
      <td><img src="../assets/deeplense/classification/confusion_matrix.png" alt="Confusion Matrix"></td>
    </tr>
    <tr>
      <td><img src="../assets/deeplense/classification/pca_plot.png" alt="PCA"></td>
      <td><img src="../assets/deeplense/classification/tsne_plot.png" alt="t-SNE"></td>
    </tr>
  </table>

  <h4>3Ô∏è‚É£ Super-Resolution</h4>
  <table>
    <tr>
      <td><img src="../assets/deeplense/superresolution/SSIM.png" alt="SSIM"></td>
      <td><img src="../assets/deeplense/superresolution/PSNR.png" alt="PSNR"></td>
    </tr>
  </table>
  <img src="../assets/deeplense/superresolution/MAE_Losses.png" alt="MSE">

  <h5>Final Metrics</h5>
  <ul>
    <li>Final Validation MSE: 0.002293</li>
    <li>Final Validation PSNR: 29.62</li>
    <li>Final Validation SSIM: 0.9190</li>
  </ul>

  <h5>Interpretation</h5>
  <ul>
    <li>Lower <strong>MSE</strong> ‚Üí Better reconstruction (less error).</li>
    <li>Higher <strong>PSNR</strong> ‚Üí Better quality. (30‚Äì50 dB ‚Üí Good | 20‚Äì30 ‚Üí Moderate | <20 ‚Üí Poor)</li>
    <li><strong>SSIM = 1</strong> ‚Üí Identical images. SSIM ‚âà 0 ‚Üí No structural similarity.</li>
    <li><strong>SSIM</strong> aligns more with human perception than MSE or PSNR.</li>
  </ul>

  <h5>Super-resolution Comparison</h5>
  <table>
    <tr>
      <td><img src="../assets/deeplense/superresolution/lr_image.png" alt="Low-Res"></td>
      <td><img src="../assets/deeplense/superresolution/superResoluted.png" alt="Predicted High-Res"></td>
      <td><img src="../assets/deeplense/superresolution/hr_image.png" alt="High-Res"></td>
    </tr>
  </table>

</body>
</html>